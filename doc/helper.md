# ğŸ“š helper.py è©³ç´°ä»•æ§˜æ›¸

## ğŸŒŸ å…¨ä½“æ¦‚è¦

OpenAI APIå­¦ç¿’ç”¨Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ã™ã€‚è¨­å®šç®¡ç†ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ã€UIæ§‹ç¯‰ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†ãªã©ã®å…±é€šæ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

| é …ç›® | å†…å®¹ |
|------|------|
| **å¯¾å¿œAPI** | OpenAI Responses API |
| **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯** | Streamlit |
| **ä¸»è¦æ©Ÿèƒ½** | è¨­å®šç®¡ç†ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç®¡ç†ã€UIæ§‹ç¯‰ã€ãƒˆãƒ¼ã‚¯ãƒ³è¨ˆç®—ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç† |
| **å¯¾å¿œãƒ¢ãƒ‡ãƒ«** | GPT-4o, GPT-4o-mini, O1, O3, O4ã‚·ãƒªãƒ¼ã‚ºç­‰ |

---

## ğŸ—ï¸ 1. ã‚¯ãƒ©ã‚¹

### ğŸ”§ 1.1 ConfigManager

> **æ¦‚è¦ï¼š** YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ»ç®¡ç†ã‚¯ãƒ©ã‚¹

| åŸºæœ¬æƒ…å ± | è©³ç´° |
|----------|------|
| **ç”¨é€”** | è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç† |
| **ã‚­ãƒ£ãƒƒã‚·ãƒ¥** | å¯¾å¿œ |
| **è¨­å®šå½¢å¼** | YAML |
| **éšå±¤ã‚¢ã‚¯ã‚»ã‚¹** | ãƒ‰ãƒƒãƒˆè¨˜æ³•å¯¾å¿œ |

#### ğŸ“‹ ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰

| ãƒ¡ã‚½ãƒƒãƒ‰ | å¼•æ•° | æˆ»ã‚Šå€¤ | èª¬æ˜ |
|----------|------|--------|------|
| `__init__()` | `config_path: str` | - | åˆæœŸåŒ– |
| `get()` | `key: str, default: Any` | `Any` | è¨­å®šå€¤å–å¾— |
| `reload()` | - | - | è¨­å®šå†èª­ã¿è¾¼ã¿ |

#### ğŸ”„ å‡¦ç†ãƒ•ãƒ­ãƒ¼

```mermaid
graph LR
    A[YAMLèª­ã¿è¾¼ã¿] --> B[ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª]
    B --> C[è¨­å®šå€¤å–å¾—]
    C --> D[ãƒ‰ãƒƒãƒˆè¨˜æ³•è§£æ]
    D --> E[çµæœè¿”å´]
```

#### ğŸ’¡ ä½¿ç”¨ä¾‹

```python
# è¨­å®šå–å¾—
config = ConfigManager()
model = config.get("models.default", "gpt-4o-mini")
timeout = config.get("api.timeout", 30)
```

---

### ğŸ’¬ 1.2 MessageManager

> **æ¦‚è¦ï¼š** OpenAI Responses APIç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ç®¡ç†ã‚¯ãƒ©ã‚¹

| åŸºæœ¬æƒ…å ± | è©³ç´° |
|----------|------|
| **å¯¾å¿œãƒ­ãƒ¼ãƒ«** | user, assistant, system, developer |
| **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸** | Streamlit SessionState |
| **åˆ¶é™æ©Ÿèƒ½** | ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°åˆ¶é™å¯¾å¿œ |
| **ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ** | JSONå½¢å¼å¯¾å¿œ |

#### ğŸ“‹ ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰

| ãƒ¡ã‚½ãƒƒãƒ‰ | å¼•æ•° | æˆ»ã‚Šå€¤ | èª¬æ˜ |
|----------|------|--------|------|
| `add_message()` | `role: RoleType, content: str` | - | ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ  |
| `get_messages()` | - | `List[EasyInputMessageParam]` | ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å–å¾— |
| `clear_messages()` | - | - | å±¥æ­´ã‚¯ãƒªã‚¢ |
| `export_messages()` | - | `Dict[str, Any]` | ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ |
| `import_messages()` | `data: Dict[str, Any]` | - | ã‚¤ãƒ³ãƒãƒ¼ãƒˆ |

#### ğŸ¯ ãƒ­ãƒ¼ãƒ«å‹å®šç¾©

| ãƒ­ãƒ¼ãƒ« | ç”¨é€” | ä½¿ç”¨API |
|--------|------|---------|
| `user` | ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ | å…±é€š |
| `assistant` | AIå¿œç­” | å…±é€š |
| `system` | ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ | ChatCompletions |
| `developer` | é–‹ç™ºè€…æŒ‡ç¤º | Responses |

#### ğŸ’¡ ä½¿ç”¨ä¾‹

```python
# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç®¡ç†
manager = MessageManager("chat_history")
manager.add_message("user", "ã“ã‚“ã«ã¡ã¯")
messages = manager.get_messages()
```

---

### ğŸ”¢ 1.3 TokenManager

> **æ¦‚è¦ï¼š** ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—ãƒ»ç®¡ç†ã‚¯ãƒ©ã‚¹

| åŸºæœ¬æƒ…å ± | è©³ç´° |
|----------|------|
| **ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°** | cl100k_base |
| **å¯¾å¿œãƒ¢ãƒ‡ãƒ«** | GPT-4oç³», O1ç³», O3ç³», O4ç³» |
| **æ©Ÿèƒ½** | ãƒˆãƒ¼ã‚¯ãƒ³è¨ˆç®—ã€ã‚³ã‚¹ãƒˆæ¨å®šã€åˆ¶é™ç¢ºèª |
| **ãƒ©ã‚¤ãƒ–ãƒ©ãƒª** | tiktoken |

#### ğŸ“‹ ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰

| ãƒ¡ã‚½ãƒƒãƒ‰ | å¼•æ•° | æˆ»ã‚Šå€¤ | èª¬æ˜ |
|----------|------|--------|------|
| `count_tokens()` | `text: str, model: str` | `int` | ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®— |
| `truncate_text()` | `text: str, max_tokens: int, model: str` | `str` | ãƒ†ã‚­ã‚¹ãƒˆåˆ‡ã‚Šè©°ã‚ |
| `estimate_cost()` | `input_tokens: int, output_tokens: int, model: str` | `float` | ã‚³ã‚¹ãƒˆæ¨å®š |
| `get_model_limits()` | `model: str` | `Dict[str, int]` | ãƒ¢ãƒ‡ãƒ«åˆ¶é™å–å¾— |

#### ğŸ·ï¸ ã‚µãƒãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«

| ãƒ¢ãƒ‡ãƒ«ç³»çµ± | æœ€å¤§å…¥åŠ› | æœ€å¤§å‡ºåŠ› | ç‰¹å¾´ |
|------------|----------|----------|------|
| **GPT-4o** | 128K | 4K | æ¨™æº–ãƒ¢ãƒ‡ãƒ« |
| **GPT-4o-mini** | 128K | 4K | è»½é‡ãƒ»é«˜é€Ÿ |
| **O1ç³»** | 128K | 32K-64K | æ¨è«–ç‰¹åŒ– |
| **O3ç³»** | 200K | 100K | é«˜æ€§èƒ½æ¨è«– |
| **O4ç³»** | 256K | 128K | æœ€æ–°ä¸–ä»£ |

#### ğŸ’¡ ä½¿ç”¨ä¾‹

```python
# ãƒˆãƒ¼ã‚¯ãƒ³ç®¡ç†
tokens = TokenManager.count_tokens("ãƒ†ã‚¹ãƒˆæ–‡ç« ", "gpt-4o-mini")
cost = TokenManager.estimate_cost(1000, 500, "gpt-4o-mini")
limits = TokenManager.get_model_limits("gpt-4o")
```

---

### ğŸ¨ 1.4 UIHelper

> **æ¦‚è¦ï¼š** Streamlit UIæ§‹ç¯‰ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹

| åŸºæœ¬æƒ…å ± | è©³ç´° |
|----------|------|
| **å¯¾è±¡** | Streamlit UI |
| **æä¾›æ©Ÿèƒ½** | ãƒšãƒ¼ã‚¸åˆæœŸåŒ–ã€ãƒ¢ãƒ‡ãƒ«é¸æŠã€è¡¨ç¤ºæ©Ÿèƒ½ |
| **å¯¾å¿œãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥** | ãƒ†ã‚­ã‚¹ãƒˆã€éŸ³å£°åˆæˆã€éŸ³å£°èªè­˜ |

#### ğŸ“‹ ãƒšãƒ¼ã‚¸ãƒ»åŸºæœ¬UI

| ãƒ¡ã‚½ãƒƒãƒ‰ | å¼•æ•° | æˆ»ã‚Šå€¤ | èª¬æ˜ |
|----------|------|--------|------|
| `init_page()` | `title: str, sidebar_title: str` | - | ãƒšãƒ¼ã‚¸åˆæœŸåŒ– |
| `create_tabs()` | `tab_names: List[str], key: str` | `List[Any]` | ã‚¿ãƒ–ä½œæˆ |
| `create_columns()` | `spec: List[Union[int, float]], gap: str` | `List[Any]` | ã‚«ãƒ©ãƒ ä½œæˆ |
| `show_metrics()` | `metrics: Dict[str, Any], columns: int` | - | ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º |

#### ğŸ“‹ ãƒ¢ãƒ‡ãƒ«é¸æŠUI

| ãƒ¡ã‚½ãƒƒãƒ‰ | å¯¾è±¡ | ã‚«ãƒ†ã‚´ãƒª | æˆ»ã‚Šå€¤ |
|----------|------|----------|--------|
| `select_model()` | ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ | standard, reasoning | `str` |
| `select_speech_model()` | éŸ³å£°åˆæˆ | tts, audio_chat, reasoning | `str` |
| `select_whisper_model()` | éŸ³å£°èªè­˜ | whisper, transcribe, audio_chat | `str` |

#### ğŸ“‹ è¡¨ç¤ºãƒ»æƒ…å ±UI

| ãƒ¡ã‚½ãƒƒãƒ‰ | å¼•æ•° | æ©Ÿèƒ½ |
|----------|------|------|
| `display_messages()` | `messages: List, show_system: bool` | ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´è¡¨ç¤º |
| `show_token_info()` | `text: str, model: str` | ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±è¡¨ç¤º |
| `create_download_button()` | `data: Any, filename: str` | ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ |

#### ğŸµ éŸ³å£°ãƒ¢ãƒ‡ãƒ«å¯¾å¿œ

| ã‚«ãƒ†ã‚´ãƒª | ãƒ¢ãƒ‡ãƒ«ä¾‹ | ç”¨é€” |
|----------|----------|------|
| **TTS** | tts-1, tts-1-hd | éŸ³å£°åˆæˆå°‚ç”¨ |
| **éŸ³å£°å¯¾è©±** | gpt-4o-audio-preview | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯¾è©± |
| **éŸ³å£°èªè­˜** | whisper-1 | è»¢å†™ãƒ»ç¿»è¨³ |

#### ğŸ’¡ ä½¿ç”¨ä¾‹

```python
# UIæ§‹ç¯‰
UIHelper.init_page("ãƒ‡ãƒ¢ã‚¢ãƒ—ãƒª")
model = UIHelper.select_model("model_key", "reasoning")
UIHelper.show_token_info("ãƒ†ã‚­ã‚¹ãƒˆ", model)
```

---

### ğŸ“¤ 1.5 ResponseProcessor

> **æ¦‚è¦ï¼š** OpenAI Responses APIãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†ã‚¯ãƒ©ã‚¹

| åŸºæœ¬æƒ…å ± | è©³ç´° |
|----------|------|
| **å¯¾å¿œAPI** | OpenAI Responses API |
| **å‡¦ç†æ©Ÿèƒ½** | ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã€æ•´å½¢ã€è¡¨ç¤ºã€ä¿å­˜ |
| **å‡ºåŠ›å½¢å¼** | JSONã€Streamlitè¡¨ç¤º |

#### ğŸ“‹ ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰

| ãƒ¡ã‚½ãƒƒãƒ‰ | å¼•æ•° | æˆ»ã‚Šå€¤ | èª¬æ˜ |
|----------|------|--------|------|
| `extract_text()` | `response: Response` | `List[str]` | ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º |
| `format_response()` | `response: Response` | `Dict[str, Any]` | ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ•´å½¢ |
| `display_response()` | `response: Response, show_details: bool` | - | Streamlitè¡¨ç¤º |
| `save_response()` | `response: Response, filename: str` | `str` | ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ |

#### ğŸ”„ å‡¦ç†ãƒ•ãƒ­ãƒ¼

```mermaid
graph TD
    A[Responseå—ä¿¡] --> B[ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º]
    B --> C[æ§‹é€ æ•´å½¢]
    C --> D[è¡¨ç¤ºå‡¦ç†]
    D --> E[ä¿å­˜å‡¦ç†]
```

#### ğŸ’¡ ä½¿ç”¨ä¾‹

```python
# ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†
texts = ResponseProcessor.extract_text(response)
ResponseProcessor.display_response(response, show_details=True)
filepath = ResponseProcessor.save_response(response)
```

---

### ğŸ›ï¸ 1.6 DemoBase

> **æ¦‚è¦ï¼š** ãƒ‡ãƒ¢ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹

| åŸºæœ¬æƒ…å ± | è©³ç´° |
|----------|------|
| **ç¶™æ‰¿æ–¹å¼** | æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹ |
| **å…±é€šæ©Ÿèƒ½** | UIè¨­å®šã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç®¡ç†ã€APIå‘¼ã³å‡ºã— |
| **å¿…é ˆå®Ÿè£…** | `run()` ãƒ¡ã‚½ãƒƒãƒ‰ |

#### ğŸ“‹ ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰

| ãƒ¡ã‚½ãƒƒãƒ‰ | ç¨®åˆ¥ | å¼•æ•° | èª¬æ˜ |
|----------|------|------|------|
| `run()` | æŠ½è±¡ | - | ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆè¦å®Ÿè£…ï¼‰ |
| `setup_ui()` | å®Ÿè£…æ¸ˆã¿ | - | å…±é€šUIè¨­å®š |
| `call_api()` | å®Ÿè£…æ¸ˆã¿ | `messages: List, **kwargs` | APIå‘¼ã³å‡ºã— |
| `add_user_message()` | å®Ÿè£…æ¸ˆã¿ | `content: str` | ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ  |
| `add_assistant_message()` | å®Ÿè£…æ¸ˆã¿ | `content: str` | ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ  |

#### ğŸ¯ å®Ÿè£…ä¾‹

```python
class ChatDemo(DemoBase):
    def run(self):
        self.setup_ui()

        if prompt := st.chat_input("å…¥åŠ›"):
            self.add_user_message(prompt)
            messages = self.message_manager.get_messages()
            response = self.call_api(messages)

            if response:
                texts = ResponseProcessor.extract_text(response)
                self.add_assistant_message(texts[0])

        self.display_messages()
```

---

## ğŸ·ï¸ 2. å‹å®šç¾©

### ğŸ“ 2.1 RoleType

| é …ç›® | å†…å®¹ |
|------|------|
| **å®šç¾©** | `Literal["user", "assistant", "system", "developer"]` |
| **ç”¨é€”** | ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ«ã®å‹å®‰å…¨æ€§ç¢ºä¿ |
| **å¯¾å¿œAPI** | OpenAI Chat Completions / Responses |

```python
RoleType = Literal["user", "assistant", "system", "developer"]
```

---

## ğŸ­ 3. ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿

### ğŸ›¡ï¸ 3.1 error_handler

| é …ç›® | å†…å®¹ |
|------|------|
| **ç”¨é€”** | ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚° |
| **è¡¨ç¤º** | Streamlitã‚¨ãƒ©ãƒ¼è¡¨ç¤º |
| **ãƒ­ã‚°** | è‡ªå‹•ãƒ­ã‚°å‡ºåŠ› |
| **ãƒ‡ãƒãƒƒã‚°** | è©³ç´°ã‚¨ãƒ©ãƒ¼è¡¨ç¤ºå¯¾å¿œ |

### â±ï¸ 3.2 timer

| é …ç›® | å†…å®¹ |
|------|------|
| **ç”¨é€”** | å®Ÿè¡Œæ™‚é–“è¨ˆæ¸¬ |
| **è¨˜éŒ²** | SessionStateã«ä¿å­˜ |
| **ç›£è¦–** | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦– |
| **ãƒ­ã‚°** | å®Ÿè¡Œæ™‚é–“ãƒ­ã‚°å‡ºåŠ› |

### ğŸ’¾ 3.3 cache_result

| é …ç›® | å†…å®¹ |
|------|------|
| **ç”¨é€”** | çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥ |
| **TTL** | æ™‚é–“åˆ¶é™å¯¾å¿œ |
| **åˆ¶é™** | ã‚µã‚¤ã‚ºåˆ¶é™ç®¡ç† |
| **ã‚­ãƒ¼** | MD5ãƒãƒƒã‚·ãƒ¥ä½¿ç”¨ |

#### ğŸ’¡ ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ä½¿ç”¨ä¾‹

```python
@error_handler
@timer
@cache_result(ttl=3600)
def expensive_function():
    # æ™‚é–“ã®ã‹ã‹ã‚‹å‡¦ç†
    return result
```

---

## ğŸ”§ 4. ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

### ğŸ“‹ æ±ç”¨é–¢æ•°ä¸€è¦§

| é–¢æ•° | å¼•æ•° | æˆ»ã‚Šå€¤ | ç”¨é€” |
|------|------|--------|------|
| `sanitize_key()` | `name: str` | `str` | Streamlit keyæ–‡å­—åˆ—åŒ– |
| `load_json_file()` | `filepath: str` | `Optional[Dict]` | JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ |
| `save_json_file()` | `data: Dict, filepath: str` | `bool` | JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ |
| `format_timestamp()` | `timestamp: Union[int, float, str]` | `str` | ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ•´å½¢ |
| `create_session_id()` | - | `str` | ã‚»ãƒƒã‚·ãƒ§ãƒ³IDç”Ÿæˆ |

#### ğŸ”„ å‡¦ç†è©³ç´°

| é–¢æ•° | å‡¦ç†å†…å®¹ |
|------|----------|
| **sanitize_key** | æ­£è¦è¡¨ç¾ `[^0-9a-zA-Z_]` â†’ `_` + å°æ–‡å­—å¤‰æ› |
| **load_json_file** | ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ â†’ JSONè§£æ â†’ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚° |
| **save_json_file** | ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ â†’ JSONæ›¸ãè¾¼ã¿ â†’ æˆåŠŸ/å¤±æ•—åˆ¤å®š |
| **format_timestamp** | å‹åˆ¤å®š â†’ datetimeå¤‰æ› â†’ `%Y-%m-%d %H:%M:%S` å½¢å¼ |
| **create_session_id** | æ™‚åˆ»+ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆID â†’ MD5ãƒãƒƒã‚·ãƒ¥ â†’ 8æ–‡å­—åˆ‡ã‚Šå‡ºã— |

---

## ğŸ”„ 5. å¾Œæ–¹äº’æ›æ€§

### ğŸ“‹ äº’æ›é–¢æ•°ä¸€è¦§

| é–¢æ•° | å§”è­²å…ˆ | ç”¨é€” |
|------|--------|------|
| `init_page()` | `UIHelper.init_page()` | ãƒšãƒ¼ã‚¸åˆæœŸåŒ– |
| `select_model()` | `UIHelper.select_model()` | ãƒ¢ãƒ‡ãƒ«é¸æŠ |
| `get_default_messages()` | `MessageManager.get_default_messages()` | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ |
| `extract_text_from_response()` | `ResponseProcessor.extract_text()` | ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º |

### ğŸ¯ ç‰¹æ®Šäº’æ›é–¢æ•°

#### `append_user_message()`

| å¼•æ•° | å‹ | èª¬æ˜ |
|------|-----|------|
| `append_text` | `str` | è¿½åŠ ãƒ†ã‚­ã‚¹ãƒˆ |
| `image_url` | `Optional[str]` | ç”»åƒURL |

**æˆ»ã‚Šå€¤ï¼š** `List[EasyInputMessageParam]`

**å‡¦ç†ï¼š** ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ

---

## ğŸš€ ç·åˆä½¿ç”¨ä¾‹

### ğŸ’¬ ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

```python
from helper import DemoBase, UIHelper, ResponseProcessor

class ChatApp(DemoBase):
    def __init__(self):
        super().__init__("chat_app", "AIãƒãƒ£ãƒƒãƒˆ")

    def run(self):
        # UIè¨­å®š
        self.setup_ui()

        # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
        if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¿½åŠ 
            self.add_user_message(prompt)

            # APIå‘¼ã³å‡ºã—
            messages = self.message_manager.get_messages()
            response = self.call_api(messages, max_tokens=1000)

            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†
            if response:
                texts = ResponseProcessor.extract_text(response)
                if texts:
                    self.add_assistant_message(texts[0])
                    ResponseProcessor.display_response(response)

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´è¡¨ç¤º
        self.display_messages()

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
if __name__ == "__main__":
    app = ChatApp()
    app.run()
```

### ğŸµ éŸ³å£°å¯¾å¿œã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

```python
# éŸ³å£°åˆæˆæ©Ÿèƒ½ä»˜ããƒãƒ£ãƒƒãƒˆ
speech_model = UIHelper.select_speech_model("speech_key", "tts")
whisper_model = UIHelper.select_whisper_model("whisper_key", "whisper")

# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
audio_file = st.file_uploader("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«", type=['mp3', 'wav'])
if audio_file:
    # éŸ³å£°èªè­˜å‡¦ç†
    transcription = openai_client.audio.transcriptions.create(
        model=whisper_model,
        file=audio_file
    )
    st.write(f"èªè­˜çµæœ: {transcription.text}")
```

---

## ğŸ“Š è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¾‹ï¼ˆconfig.yamlï¼‰

```yaml
models:
  default: "gpt-4o-mini"
  available:
    - "gpt-4o"
    - "gpt-4o-mini"
    - "gpt-4.1"
    - "o1"
    - "o3-mini"

api:
  timeout: 30
  max_retries: 3

ui:
  page_title: "OpenAI API Demo"
  layout: "wide"
  message_display_limit: 50

model_pricing:
  gpt-4o-mini:
    input: 0.00015
    output: 0.0006
  gpt-4o:
    input: 0.005
    output: 0.015

cache:
  enabled: true
  ttl: 3600
  max_size: 100
```

ã“ã®hepler.pyãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ã‚ˆã‚Šã€OpenAI APIã‚’ä½¿ç”¨ã—ãŸStreamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®é–‹ç™ºãŒåŠ¹ç‡çš„ã«è¡Œãˆã¾ã™ã€‚ğŸ‰